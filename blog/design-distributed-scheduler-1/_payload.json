[{"data":1,"prerenderedAt":1437},["ShallowReactive",2],{"content-query-F4qxbcqyWA":3},{"_path":4,"_dir":5,"_draft":6,"_partial":6,"_locale":7,"title":8,"description":9,"tags":10,"date":16,"image":17,"author":18,"draft":6,"slug":19,"body":20,"_type":1431,"_id":1432,"_source":1433,"_file":1434,"_stem":1435,"_extension":1436},"/blog/design-distributed-scheduler-1","blog",false,"","Design Distributed Scheduler - I","This article explores the design of a distributed scheduler optimized for scalability and fault tolerance. It covers key challenges like partitioning tasks, managing concurrency, and ensuring load distribution across multiple instances.",[11,12,13,14,15],"shard","partitions","scheduler","design","system architecture","2024-12-11","/blog/design-distributed-scheduler-1/images/thumbnail.webp","Andrea Petreti","design-distributed-scheduler-1",{"type":21,"children":22,"toc":1415},"root",[23,31,36,43,48,68,73,79,84,93,98,104,109,114,119,124,131,136,277,282,360,366,378,416,421,446,452,463,541,546,566,599,657,670,676,696,702,707,749,754,761,766,771,777,782,800,805,810,830,835,949,968,980,1037,1070,1161,1166,1171,1176,1183,1201,1219,1224,1237,1251,1280,1286,1291,1321,1340,1345,1368,1375,1381,1386,1391,1409],{"type":24,"tag":25,"props":26,"children":27},"element","p",{},[28],{"type":29,"value":30},"text","Having a distributed scheduler can be useful for orchestration purposes. For instance, it can be used to send a reminder after X minutes or enforce a workflow timeout (e.g., ensuring a payment is completed within Y minutes). However, designing a scheduler in a distributed environment is no trivial task, especially when aiming for high availability and scalability.",{"type":24,"tag":25,"props":32,"children":33},{},[34],{"type":29,"value":35},"In this design, I will focus primarily on the challenges of achieving scalability and high availability. To keep things straightforward, I will simplify the scheduler's requirements. For example, it will not handle retries for failed jobs, nor will it deal with job-specific concepts. Instead, the scheduler will operate on an abstract level with messages or commands. These messages or commands can be executed by any worker, potentially even the same worker that requested the scheduling.",{"type":24,"tag":37,"props":38,"children":40},"h2",{"id":39},"use-cases",[41],{"type":29,"value":42},"Use cases",{"type":24,"tag":25,"props":44,"children":45},{},[46],{"type":29,"value":47},"For this system let's assume the following use cases:",{"type":24,"tag":49,"props":50,"children":51},"ul",{},[52,58,63],{"type":24,"tag":53,"props":54,"children":55},"li",{},[56],{"type":29,"value":57},"user can schedule a message to be executed at given time",{"type":24,"tag":53,"props":59,"children":60},{},[61],{"type":29,"value":62},"user can cancel a scheduled message",{"type":24,"tag":53,"props":64,"children":65},{},[66],{"type":29,"value":67},"user can specify a channel (something similar to a kafka topic) where they want to receive the message",{"type":24,"tag":25,"props":69,"children":70},{},[71],{"type":29,"value":72},"Essentially, the system will function like a delayed queue, where messages are delivered at a specified time. When a message's time is up, the system will publish it to a real queue (e.g., Kafka, RabbitMQ).",{"type":24,"tag":37,"props":74,"children":76},{"id":75},"big-picture-architecture",[77],{"type":29,"value":78},"Big picture architecture",{"type":24,"tag":25,"props":80,"children":81},{},[82],{"type":29,"value":83},"The main idea is to create a scheduler service which reads pending messages at given time, submit to queue and then track it's progress, like an offset. When a message is successfully delivered to destination topic then the scheduler update the message status (ack/nack).",{"type":24,"tag":85,"props":86,"children":92},"img",{"src":87,"alt":88,"className":89,"width":91},"/blog/design-distributed-scheduler-1/images/big-picture.webp","big-picture",[90],"center-md-image",500,[],{"type":24,"tag":25,"props":94,"children":95},{},[96],{"type":29,"value":97},"Event the diagram shows multiple instance of scheduler service, this design start by\nexploring a solution based on single instance Scheduler and then starts by making some re-design\nto make system horizontally scalable.",{"type":24,"tag":37,"props":99,"children":101},{"id":100},"database",[102],{"type":29,"value":103},"Database",{"type":24,"tag":25,"props":105,"children":106},{},[107],{"type":29,"value":108},"A robust database schema is a crucial element that directly impacts overall system performance. To optimize efficiency, we can start by splitting the read and write loads based on the system’s use cases.",{"type":24,"tag":25,"props":110,"children":111},{},[112],{"type":29,"value":113},"It’s easy to see that the primary type of load on the database is “read”. The scheduler will periodically (e.g., every X minutes) query the pending scheduled messages to determine which ones need to be submitted immediately.",{"type":24,"tag":25,"props":115,"children":116},{},[117],{"type":29,"value":118},"What about the write load? From the perspective of external actors, the system supports creating new scheduled messages or deleting existing ones. Internally, the scheduler also needs to update the status of messages once they are completed or have failed.",{"type":24,"tag":25,"props":120,"children":121},{},[122],{"type":29,"value":123},"The key challenge is to design an efficient way to retrieve messages that are ready to be submitted. For this design, I’ll use MongoDB, a NoSQL database that enables easy scaling and partitioning of data.",{"type":24,"tag":125,"props":126,"children":128},"h3",{"id":127},"schema-design",[129],{"type":29,"value":130},"Schema design",{"type":24,"tag":25,"props":132,"children":133},{},[134],{"type":29,"value":135},"A database schema for NoSQL like database",{"type":24,"tag":137,"props":138,"children":142},"pre",{"className":139,"code":140,"language":141,"meta":7,"style":7},"language-json shiki shiki-themes github-dark-dimmed","{\n    \"scheduleId\": \"...\",\n    \"destinationTopic\": \"...\",\n    \"executionTime\": \"date-time\",\n    \"payload\": \"scheduled command payload\",\n    \"status\": \"PENDING|DELIVERED|DELIVERY_FAILED\"\n}\n","json",[143],{"type":24,"tag":144,"props":145,"children":146},"code",{"__ignoreMap":7},[147,159,185,206,228,250,268],{"type":24,"tag":148,"props":149,"children":152},"span",{"class":150,"line":151},"line",1,[153],{"type":24,"tag":148,"props":154,"children":156},{"style":155},"--shiki-default:#ADBAC7",[157],{"type":29,"value":158},"{\n",{"type":24,"tag":148,"props":160,"children":162},{"class":150,"line":161},2,[163,169,174,180],{"type":24,"tag":148,"props":164,"children":166},{"style":165},"--shiki-default:#8DDB8C",[167],{"type":29,"value":168},"    \"scheduleId\"",{"type":24,"tag":148,"props":170,"children":171},{"style":155},[172],{"type":29,"value":173},": ",{"type":24,"tag":148,"props":175,"children":177},{"style":176},"--shiki-default:#96D0FF",[178],{"type":29,"value":179},"\"...\"",{"type":24,"tag":148,"props":181,"children":182},{"style":155},[183],{"type":29,"value":184},",\n",{"type":24,"tag":148,"props":186,"children":188},{"class":150,"line":187},3,[189,194,198,202],{"type":24,"tag":148,"props":190,"children":191},{"style":165},[192],{"type":29,"value":193},"    \"destinationTopic\"",{"type":24,"tag":148,"props":195,"children":196},{"style":155},[197],{"type":29,"value":173},{"type":24,"tag":148,"props":199,"children":200},{"style":176},[201],{"type":29,"value":179},{"type":24,"tag":148,"props":203,"children":204},{"style":155},[205],{"type":29,"value":184},{"type":24,"tag":148,"props":207,"children":209},{"class":150,"line":208},4,[210,215,219,224],{"type":24,"tag":148,"props":211,"children":212},{"style":165},[213],{"type":29,"value":214},"    \"executionTime\"",{"type":24,"tag":148,"props":216,"children":217},{"style":155},[218],{"type":29,"value":173},{"type":24,"tag":148,"props":220,"children":221},{"style":176},[222],{"type":29,"value":223},"\"date-time\"",{"type":24,"tag":148,"props":225,"children":226},{"style":155},[227],{"type":29,"value":184},{"type":24,"tag":148,"props":229,"children":231},{"class":150,"line":230},5,[232,237,241,246],{"type":24,"tag":148,"props":233,"children":234},{"style":165},[235],{"type":29,"value":236},"    \"payload\"",{"type":24,"tag":148,"props":238,"children":239},{"style":155},[240],{"type":29,"value":173},{"type":24,"tag":148,"props":242,"children":243},{"style":176},[244],{"type":29,"value":245},"\"scheduled command payload\"",{"type":24,"tag":148,"props":247,"children":248},{"style":155},[249],{"type":29,"value":184},{"type":24,"tag":148,"props":251,"children":253},{"class":150,"line":252},6,[254,259,263],{"type":24,"tag":148,"props":255,"children":256},{"style":165},[257],{"type":29,"value":258},"    \"status\"",{"type":24,"tag":148,"props":260,"children":261},{"style":155},[262],{"type":29,"value":173},{"type":24,"tag":148,"props":264,"children":265},{"style":176},[266],{"type":29,"value":267},"\"PENDING|DELIVERED|DELIVERY_FAILED\"\n",{"type":24,"tag":148,"props":269,"children":271},{"class":150,"line":270},7,[272],{"type":24,"tag":148,"props":273,"children":274},{"style":155},[275],{"type":29,"value":276},"}\n",{"type":24,"tag":25,"props":278,"children":279},{},[280],{"type":29,"value":281},"The schema is pretty simple with few fields:",{"type":24,"tag":49,"props":283,"children":284},{},[285,296,339,350],{"type":24,"tag":53,"props":286,"children":287},{},[288,294],{"type":24,"tag":144,"props":289,"children":291},{"className":290},[],[292],{"type":29,"value":293},"payload",{"type":29,"value":295},": Contains the data for the scheduled job. This payload will be delivered to the queue.",{"type":24,"tag":53,"props":297,"children":298},{},[299,305,307,313,315,321,323,329,331,337],{"type":24,"tag":144,"props":300,"children":302},{"className":301},[],[303],{"type":29,"value":304},"status",{"type":29,"value":306},": Tracks the schedule status. The default status is ",{"type":24,"tag":144,"props":308,"children":310},{"className":309},[],[311],{"type":29,"value":312},"PENDING",{"type":29,"value":314},", while ",{"type":24,"tag":144,"props":316,"children":318},{"className":317},[],[319],{"type":29,"value":320},"DELIVERED",{"type":29,"value":322}," means the message has been sent to the ",{"type":24,"tag":144,"props":324,"children":326},{"className":325},[],[327],{"type":29,"value":328},"destinationTopic",{"type":29,"value":330},", and ",{"type":24,"tag":144,"props":332,"children":334},{"className":333},[],[335],{"type":29,"value":336},"DELIVERY_FAILED",{"type":29,"value":338}," indicates a failure to deliver the message to the queue.",{"type":24,"tag":53,"props":340,"children":341},{},[342,348],{"type":24,"tag":144,"props":343,"children":345},{"className":344},[],[346],{"type":29,"value":347},"executionTime",{"type":29,"value":349},": A timestamp, rounded to the nearest minute or second, based on the scheduler’s granularity.",{"type":24,"tag":53,"props":351,"children":352},{},[353,358],{"type":24,"tag":144,"props":354,"children":356},{"className":355},[],[357],{"type":29,"value":328},{"type":29,"value":359},": Specifies the topic where the scheduler will publish the message. This could be a Kafka topic or a RabbitMQ routing key.",{"type":24,"tag":125,"props":361,"children":363},{"id":362},"storing-offset-and-execution-time",[364],{"type":29,"value":365},"Storing Offset and Execution Time",{"type":24,"tag":25,"props":367,"children":368},{},[369,371,376],{"type":29,"value":370},"In addition to the main scheduler collection, the system will have a dedicated collection to store the offset and the last ",{"type":24,"tag":144,"props":372,"children":374},{"className":373},[],[375],{"type":29,"value":347},{"type":29,"value":377},". This will allow the scheduler to track the progress of scheduled messages more effectively.\nThe structure of this collection could look like this:",{"type":24,"tag":137,"props":379,"children":381},{"className":139,"code":380,"language":141,"meta":7,"style":7},"{\n    \"lastExecutionTime\": \"date-time\"\n}\n",[382],{"type":24,"tag":144,"props":383,"children":384},{"__ignoreMap":7},[385,392,409],{"type":24,"tag":148,"props":386,"children":387},{"class":150,"line":151},[388],{"type":24,"tag":148,"props":389,"children":390},{"style":155},[391],{"type":29,"value":158},{"type":24,"tag":148,"props":393,"children":394},{"class":150,"line":161},[395,400,404],{"type":24,"tag":148,"props":396,"children":397},{"style":165},[398],{"type":29,"value":399},"    \"lastExecutionTime\"",{"type":24,"tag":148,"props":401,"children":402},{"style":155},[403],{"type":29,"value":173},{"type":24,"tag":148,"props":405,"children":406},{"style":176},[407],{"type":29,"value":408},"\"date-time\"\n",{"type":24,"tag":148,"props":410,"children":411},{"class":150,"line":187},[412],{"type":24,"tag":148,"props":413,"children":414},{"style":155},[415],{"type":29,"value":276},{"type":24,"tag":25,"props":417,"children":418},{},[419],{"type":29,"value":420},"Where:",{"type":24,"tag":49,"props":422,"children":423},{},[424,435],{"type":24,"tag":53,"props":425,"children":426},{},[427,433],{"type":24,"tag":144,"props":428,"children":430},{"className":429},[],[431],{"type":29,"value":432},"lastTaskId",{"type":29,"value":434},": A unique identifier for the task being tracked.",{"type":24,"tag":53,"props":436,"children":437},{},[438,444],{"type":24,"tag":144,"props":439,"children":441},{"className":440},[],[442],{"type":29,"value":443},"lastExecutionTime",{"type":29,"value":445},": The last time the task was executed, helping to track the progress and avoid redundant processing.",{"type":24,"tag":125,"props":447,"children":449},{"id":448},"optimize-queries",[450],{"type":29,"value":451},"Optimize queries",{"type":24,"tag":25,"props":453,"children":454},{},[455,457],{"type":29,"value":456},"Example of SQL-like query to retrieve messages for next execution time. e.g. current ",{"type":24,"tag":144,"props":458,"children":460},{"className":459},[],[461],{"type":29,"value":462},"executionTime > 2024/09/25T12:50:00Z AND executionTime \u003C 2024/09/25T12:50:00Z",{"type":24,"tag":137,"props":464,"children":468},{"className":465,"code":466,"language":467,"meta":7,"style":7},"language-sql shiki shiki-themes github-dark-dimmed","SELECT * FROM schedules \nWHERE executionTime > {lastExecutionTime} \nAND executionTime \u003C \"2024/09/25T12:50:01Z\" \n","sql",[469],{"type":24,"tag":144,"props":470,"children":471},{"__ignoreMap":7},[472,496,519],{"type":24,"tag":148,"props":473,"children":474},{"class":150,"line":151},[475,481,486,491],{"type":24,"tag":148,"props":476,"children":478},{"style":477},"--shiki-default:#F47067",[479],{"type":29,"value":480},"SELECT",{"type":24,"tag":148,"props":482,"children":483},{"style":477},[484],{"type":29,"value":485}," *",{"type":24,"tag":148,"props":487,"children":488},{"style":477},[489],{"type":29,"value":490}," FROM",{"type":24,"tag":148,"props":492,"children":493},{"style":155},[494],{"type":29,"value":495}," schedules \n",{"type":24,"tag":148,"props":497,"children":498},{"class":150,"line":161},[499,504,509,514],{"type":24,"tag":148,"props":500,"children":501},{"style":477},[502],{"type":29,"value":503},"WHERE",{"type":24,"tag":148,"props":505,"children":506},{"style":155},[507],{"type":29,"value":508}," executionTime ",{"type":24,"tag":148,"props":510,"children":511},{"style":477},[512],{"type":29,"value":513},">",{"type":24,"tag":148,"props":515,"children":516},{"style":155},[517],{"type":29,"value":518}," {lastExecutionTime} \n",{"type":24,"tag":148,"props":520,"children":521},{"class":150,"line":187},[522,527,531,536],{"type":24,"tag":148,"props":523,"children":524},{"style":477},[525],{"type":29,"value":526},"AND",{"type":24,"tag":148,"props":528,"children":529},{"style":155},[530],{"type":29,"value":508},{"type":24,"tag":148,"props":532,"children":533},{"style":477},[534],{"type":29,"value":535},"\u003C",{"type":24,"tag":148,"props":537,"children":538},{"style":176},[539],{"type":29,"value":540}," \"2024/09/25T12:50:01Z\"\n",{"type":24,"tag":25,"props":542,"children":543},{},[544],{"type":29,"value":545},"Why use a range query? It makes handling issues related to timestamp precision or small variations in service processing easier, especially in failure scenarios.",{"type":24,"tag":25,"props":547,"children":548},{},[549,551,557,559,564],{"type":29,"value":550},"Without a proper data partition strategy, this query could result in a full scan of all shards. For example, if using ",{"type":24,"tag":144,"props":552,"children":554},{"className":553},[],[555],{"type":29,"value":556},"scheduleId",{"type":29,"value":558}," as the shard key, running this query would span multiple partitions. To optimize the search query, a better shard key would be ",{"type":24,"tag":144,"props":560,"children":562},{"className":561},[],[563],{"type":29,"value":347},{"type":29,"value":565},". This approach allows the query to be answered by exploring a single shard or a reduced number of shards. In MongoDB, this type of shard key is called a Range Shard Key. MongoDB automatically splits the data space into multiple ranges and redirects the request to the most appropriate shard or set of shards. From an abstract point of view, it's as if we are grouping all jobs with the same scheduling date into the same group.",{"type":24,"tag":25,"props":567,"children":568},{},[569,571,576,578,583,585,590,592,597],{"type":29,"value":570},"The second operation involves updating the message status to either ",{"type":24,"tag":144,"props":572,"children":574},{"className":573},[],[575],{"type":29,"value":320},{"type":29,"value":577}," or ",{"type":24,"tag":144,"props":579,"children":581},{"className":580},[],[582],{"type":29,"value":336},{"type":29,"value":584},".\nThis operation will use ",{"type":24,"tag":144,"props":586,"children":588},{"className":587},[],[589],{"type":29,"value":556},{"type":29,"value":591}," as the unique identifier. However, if we only use ",{"type":24,"tag":144,"props":593,"children":595},{"className":594},[],[596],{"type":29,"value":347},{"type":29,"value":598}," as the shard key,\nthe write operation may lead to a scatter-gather query, which can reduce efficiency.\nTo improve performance, we can create a composite shard key by combining two keys.\nThis allows for more efficient updates by directing the operation to a specific shard.",{"type":24,"tag":137,"props":600,"children":604},{"className":601,"code":602,"language":603,"meta":7,"style":7},"language-js shiki shiki-themes github-dark-dimmed","db.shardCollection(\"messages\", {executionTime: 1, scheduleId: 1})\n","js",[605],{"type":24,"tag":144,"props":606,"children":607},{"__ignoreMap":7},[608],{"type":24,"tag":148,"props":609,"children":610},{"class":150,"line":151},[611,616,622,627,632,637,643,648,652],{"type":24,"tag":148,"props":612,"children":613},{"style":155},[614],{"type":29,"value":615},"db.",{"type":24,"tag":148,"props":617,"children":619},{"style":618},"--shiki-default:#DCBDFB",[620],{"type":29,"value":621},"shardCollection",{"type":24,"tag":148,"props":623,"children":624},{"style":155},[625],{"type":29,"value":626},"(",{"type":24,"tag":148,"props":628,"children":629},{"style":176},[630],{"type":29,"value":631},"\"messages\"",{"type":24,"tag":148,"props":633,"children":634},{"style":155},[635],{"type":29,"value":636},", {executionTime: ",{"type":24,"tag":148,"props":638,"children":640},{"style":639},"--shiki-default:#6CB6FF",[641],{"type":29,"value":642},"1",{"type":24,"tag":148,"props":644,"children":645},{"style":155},[646],{"type":29,"value":647},", scheduleId: ",{"type":24,"tag":148,"props":649,"children":650},{"style":639},[651],{"type":29,"value":642},{"type":24,"tag":148,"props":653,"children":654},{"style":155},[655],{"type":29,"value":656},"})\n",{"type":24,"tag":25,"props":658,"children":659},{},[660,662,668],{"type":29,"value":661},"With this composite shard key, the MongoDB query planner will target a single shard for each operation,\nimproving the performance and efficiency of writes. The first state in the query planner would be ",{"type":24,"tag":144,"props":663,"children":665},{"className":664},[],[666],{"type":29,"value":667},"SINGLE_SHARD",{"type":29,"value":669},",\nmeaning the query no longer needs to perform a scatter-gather across multiple shards.",{"type":24,"tag":125,"props":671,"children":673},{"id":672},"dealing-with-skewed-partitions-and-hotspotting",[674],{"type":29,"value":675},"Dealing with Skewed Partitions and Hotspotting",{"type":24,"tag":25,"props":677,"children":678},{},[679,681,687,689,694],{"type":29,"value":680},"What happens if many jobs are scheduled at the same time?\nIt depends on the granularity of the executionTime (e.g., rounding the timestamp to the second or minute),\nbut it could lead to ",{"type":24,"tag":682,"props":683,"children":684},"strong",{},[685],{"type":29,"value":686},"skewed partitions",{"type":29,"value":688},".\nMongoDB also faces issues with Monotonic Increasing Keys, like timestamps, which can cause shard hotspotting.\nThis happens when new data constantly gets routed to the same shard due to a sequence of timestamps being close to each other.\nOver time, this leads to uneven load distribution and potential performance bottlenecks.\nWe will address how to resolve this issue in the next section when discussing the ",{"type":24,"tag":682,"props":690,"children":691},{},[692],{"type":29,"value":693},"scheduler-service",{"type":29,"value":695},".",{"type":24,"tag":37,"props":697,"children":699},{"id":698},"design-scheduler-service",[700],{"type":29,"value":701},"Design Scheduler service",{"type":24,"tag":25,"props":703,"children":704},{},[705],{"type":29,"value":706},"In this first scheduler design, it's behaviour is quite  simple.",{"type":24,"tag":708,"props":709,"children":710},"ol",{},[711,724,734,739,744],{"type":24,"tag":53,"props":712,"children":713},{},[714,716,722],{"type":29,"value":715},"polling every minute/second for messages with ",{"type":24,"tag":144,"props":717,"children":719},{"className":718},[],[720],{"type":29,"value":721},"scheduledTime",{"type":29,"value":723}," after last offeset",{"type":24,"tag":53,"props":725,"children":726},{},[727,729],{"type":29,"value":728},"check if message is ",{"type":24,"tag":144,"props":730,"children":732},{"className":731},[],[733],{"type":29,"value":312},{"type":24,"tag":53,"props":735,"children":736},{},[737],{"type":29,"value":738},"send a message to queue with payload",{"type":24,"tag":53,"props":740,"children":741},{},[742],{"type":29,"value":743},"update job status based on queue’s publish outcome",{"type":24,"tag":53,"props":745,"children":746},{},[747],{"type":29,"value":748},"checkpoint the offset by updating document.",{"type":24,"tag":25,"props":750,"children":751},{},[752],{"type":29,"value":753},"Optionally: To improve performance and reduce writes to the database, checkpointing can be performed every X seconds.\nThe downside of this approach is that the service could crash without writing its last checkpoint.\nWhen the service restarts, it will re-read the previous batch of messages,\nbut it won't produce duplicate messages due to the message status field.",{"type":24,"tag":85,"props":755,"children":760},{"src":756,"alt":757,"className":758,"width":759},"/blog/design-distributed-scheduler-1/images/sequence.svg","sequence",[90],600,[],{"type":24,"tag":25,"props":762,"children":763},{},[764],{"type":29,"value":765},"Obviously, the polling frequency determines the scheduler’s granularity.\nFor now, let's assume second-level granularity, even though this implies a lot of queries to the database.\nIn subsequent designs, I will explore ways to improve the system and reduce unnecessary queries\n(spoiler: using an actor-like approach and caching).",{"type":24,"tag":25,"props":767,"children":768},{},[769],{"type":29,"value":770},"The system seems ready for implementation, but what about horizontal scalability?\nIf I want to run multiple instances of the scheduler, this could lead to concurrency issues.\nFor example, two different instances might try to poll the same jobs and send the same events.\nAdditionally, there is no load distribution between instances!",{"type":24,"tag":37,"props":772,"children":774},{"id":773},"re-design-make-it-scalable",[775],{"type":29,"value":776},"Re-Design: Make it scalable!",{"type":24,"tag":25,"props":778,"children":779},{},[780],{"type":29,"value":781},"Let's make a little bit of redesign to address some issues:",{"type":24,"tag":49,"props":783,"children":784},{},[785,790,795],{"type":24,"tag":53,"props":786,"children":787},{},[788],{"type":29,"value":789},"High load for single worker when there are a lot of scheduled job at same second",{"type":24,"tag":53,"props":791,"children":792},{},[793],{"type":29,"value":794},"The scheduler service cannot properly scale horizontally. Increasing instances doesn't distribute the load.",{"type":24,"tag":53,"props":796,"children":797},{},[798],{"type":29,"value":799},"The scheduler service is a single point of failure.",{"type":24,"tag":25,"props":801,"children":802},{},[803],{"type":29,"value":804},"The goal is to distribute the load at the application level. For example, if you have 10,000 jobs scheduled at the same time, we can split them into 10,000 / number of instances.\nThis way, each service will handle a different partition and poll only for its assigned partitions.",{"type":24,"tag":125,"props":806,"children":808},{"id":807},"database-1",[809],{"type":29,"value":103},{"type":24,"tag":25,"props":811,"children":812},{},[813,815,821,823,828],{"type":29,"value":814},"At the database schema level, we need to introduce a new field: ",{"type":24,"tag":144,"props":816,"children":818},{"className":817},[],[819],{"type":29,"value":820},"bucket",{"type":29,"value":822},".\nEach bucket can contain multiple scheduled messages at the same second. Suppose we define a total number of buckets (e.g. 20),\nthis means that messages scheduled at the same time can potentially be processed by a maximum of ",{"type":24,"tag":682,"props":824,"children":825},{},[826],{"type":29,"value":827},"20 parallel instances",{"type":29,"value":829},".\nSo, the number of buckets is a hyperparameter that should be fine-tuned based on load expectations, something\nsimilar to the number of partitions in a Kafka topic.",{"type":24,"tag":25,"props":831,"children":832},{},[833],{"type":29,"value":834},"So extend the schema in the following way:",{"type":24,"tag":137,"props":836,"children":838},{"className":139,"code":837,"language":141,"meta":7,"style":7},"{ \n    \"scheduleId\": \"...\",\n    \"executionTime\": \"date-time\",\n    \"bucket\": 1,\n    \"payload\": \"scheduled command payload\",\n    \"status\": \"PENDING|DELIVERED|DELIVERY_FAILED\"\n}\n",[839],{"type":24,"tag":144,"props":840,"children":841},{"__ignoreMap":7},[842,850,869,888,908,927,942],{"type":24,"tag":148,"props":843,"children":844},{"class":150,"line":151},[845],{"type":24,"tag":148,"props":846,"children":847},{"style":155},[848],{"type":29,"value":849},"{ \n",{"type":24,"tag":148,"props":851,"children":852},{"class":150,"line":161},[853,857,861,865],{"type":24,"tag":148,"props":854,"children":855},{"style":165},[856],{"type":29,"value":168},{"type":24,"tag":148,"props":858,"children":859},{"style":155},[860],{"type":29,"value":173},{"type":24,"tag":148,"props":862,"children":863},{"style":176},[864],{"type":29,"value":179},{"type":24,"tag":148,"props":866,"children":867},{"style":155},[868],{"type":29,"value":184},{"type":24,"tag":148,"props":870,"children":871},{"class":150,"line":187},[872,876,880,884],{"type":24,"tag":148,"props":873,"children":874},{"style":165},[875],{"type":29,"value":214},{"type":24,"tag":148,"props":877,"children":878},{"style":155},[879],{"type":29,"value":173},{"type":24,"tag":148,"props":881,"children":882},{"style":176},[883],{"type":29,"value":223},{"type":24,"tag":148,"props":885,"children":886},{"style":155},[887],{"type":29,"value":184},{"type":24,"tag":148,"props":889,"children":890},{"class":150,"line":208},[891,896,900,904],{"type":24,"tag":148,"props":892,"children":893},{"style":165},[894],{"type":29,"value":895},"    \"bucket\"",{"type":24,"tag":148,"props":897,"children":898},{"style":155},[899],{"type":29,"value":173},{"type":24,"tag":148,"props":901,"children":902},{"style":639},[903],{"type":29,"value":642},{"type":24,"tag":148,"props":905,"children":906},{"style":155},[907],{"type":29,"value":184},{"type":24,"tag":148,"props":909,"children":910},{"class":150,"line":230},[911,915,919,923],{"type":24,"tag":148,"props":912,"children":913},{"style":165},[914],{"type":29,"value":236},{"type":24,"tag":148,"props":916,"children":917},{"style":155},[918],{"type":29,"value":173},{"type":24,"tag":148,"props":920,"children":921},{"style":176},[922],{"type":29,"value":245},{"type":24,"tag":148,"props":924,"children":925},{"style":155},[926],{"type":29,"value":184},{"type":24,"tag":148,"props":928,"children":929},{"class":150,"line":252},[930,934,938],{"type":24,"tag":148,"props":931,"children":932},{"style":165},[933],{"type":29,"value":258},{"type":24,"tag":148,"props":935,"children":936},{"style":155},[937],{"type":29,"value":173},{"type":24,"tag":148,"props":939,"children":940},{"style":176},[941],{"type":29,"value":267},{"type":24,"tag":148,"props":943,"children":944},{"class":150,"line":270},[945],{"type":24,"tag":148,"props":946,"children":947},{"style":155},[948],{"type":29,"value":276},{"type":24,"tag":25,"props":950,"children":951},{},[952,954,959,961,966],{"type":29,"value":953},"We can leverage the combination of ",{"type":24,"tag":144,"props":955,"children":957},{"className":956},[],[958],{"type":29,"value":347},{"type":29,"value":960}," and ",{"type":24,"tag":144,"props":962,"children":964},{"className":963},[],[965],{"type":29,"value":820},{"type":29,"value":967}," as a composite shard key to improve and parallelize\nreads across multiple instances. Messages scheduled at the same time and in the same bucket will be colocated\nin the same MongoDB partition, which helps reduce partition skew for jobs scheduled at identical times.",{"type":24,"tag":25,"props":969,"children":970},{},[971,973,978],{"type":29,"value":972},"However, the MongoDB monotonic timestamp issue still persists. To mitigate this,\nusing ",{"type":24,"tag":144,"props":974,"children":976},{"className":975},[],[977],{"type":29,"value":820},{"type":29,"value":979}," as the shard key prefix (the first part of the composite key) can help reduce the negative effects\nof a monotonic timestamp:",{"type":24,"tag":137,"props":981,"children":983},{"className":601,"code":982,"language":603,"meta":7,"style":7},"db.shardCollection(\"messages\", {bucket: \"hashed\", executionTime: 1, scheduleId: 1})\n",[984],{"type":24,"tag":144,"props":985,"children":986},{"__ignoreMap":7},[987],{"type":24,"tag":148,"props":988,"children":989},{"class":150,"line":151},[990,994,998,1002,1006,1011,1016,1021,1025,1029,1033],{"type":24,"tag":148,"props":991,"children":992},{"style":155},[993],{"type":29,"value":615},{"type":24,"tag":148,"props":995,"children":996},{"style":618},[997],{"type":29,"value":621},{"type":24,"tag":148,"props":999,"children":1000},{"style":155},[1001],{"type":29,"value":626},{"type":24,"tag":148,"props":1003,"children":1004},{"style":176},[1005],{"type":29,"value":631},{"type":24,"tag":148,"props":1007,"children":1008},{"style":155},[1009],{"type":29,"value":1010},", {bucket: ",{"type":24,"tag":148,"props":1012,"children":1013},{"style":176},[1014],{"type":29,"value":1015},"\"hashed\"",{"type":24,"tag":148,"props":1017,"children":1018},{"style":155},[1019],{"type":29,"value":1020},", executionTime: ",{"type":24,"tag":148,"props":1022,"children":1023},{"style":639},[1024],{"type":29,"value":642},{"type":24,"tag":148,"props":1026,"children":1027},{"style":155},[1028],{"type":29,"value":647},{"type":24,"tag":148,"props":1030,"children":1031},{"style":639},[1032],{"type":29,"value":642},{"type":24,"tag":148,"props":1034,"children":1035},{"style":155},[1036],{"type":29,"value":656},{"type":24,"tag":25,"props":1038,"children":1039},{},[1040,1042,1047,1049,1054,1056,1061,1063,1068],{"type":29,"value":1041},"Even though ",{"type":24,"tag":144,"props":1043,"children":1045},{"className":1044},[],[1046],{"type":29,"value":820},{"type":29,"value":1048}," is a low-cardinality key, its combination with ",{"type":24,"tag":144,"props":1050,"children":1052},{"className":1051},[],[1053],{"type":29,"value":347},{"type":29,"value":1055}," (which is a high-cardinality and monotonic key)\nand ",{"type":24,"tag":144,"props":1057,"children":1059},{"className":1058},[],[1060],{"type":29,"value":556},{"type":29,"value":1062}," (also a high-cardinality key) gives MongoDB enough information to effectively split the shards without issues. Additionally,\nusing a \"hashed\" ",{"type":24,"tag":144,"props":1064,"children":1066},{"className":1065},[],[1067],{"type":29,"value":820},{"type":29,"value":1069}," ensures event data distribution across shards, thereby avoiding hotspotting.\nNow the query to retrieve next scheduled messages looks like the following one:",{"type":24,"tag":137,"props":1071,"children":1073},{"className":465,"code":1072,"language":467,"meta":7,"style":7},"SELECT * FROM schedules \nWHERE bucket = {schedulerInstanceAssignedBucket} AND\nWHERE executionTime > {lastExecutionTime} \nAND executionTime \u003C \"2024/09/25T12:50:01Z\"\n",[1074],{"type":24,"tag":144,"props":1075,"children":1076},{"__ignoreMap":7},[1077,1096,1123,1142],{"type":24,"tag":148,"props":1078,"children":1079},{"class":150,"line":151},[1080,1084,1088,1092],{"type":24,"tag":148,"props":1081,"children":1082},{"style":477},[1083],{"type":29,"value":480},{"type":24,"tag":148,"props":1085,"children":1086},{"style":477},[1087],{"type":29,"value":485},{"type":24,"tag":148,"props":1089,"children":1090},{"style":477},[1091],{"type":29,"value":490},{"type":24,"tag":148,"props":1093,"children":1094},{"style":155},[1095],{"type":29,"value":495},{"type":24,"tag":148,"props":1097,"children":1098},{"class":150,"line":161},[1099,1103,1108,1113,1118],{"type":24,"tag":148,"props":1100,"children":1101},{"style":477},[1102],{"type":29,"value":503},{"type":24,"tag":148,"props":1104,"children":1105},{"style":155},[1106],{"type":29,"value":1107}," bucket ",{"type":24,"tag":148,"props":1109,"children":1110},{"style":477},[1111],{"type":29,"value":1112},"=",{"type":24,"tag":148,"props":1114,"children":1115},{"style":155},[1116],{"type":29,"value":1117}," {schedulerInstanceAssignedBucket} ",{"type":24,"tag":148,"props":1119,"children":1120},{"style":477},[1121],{"type":29,"value":1122},"AND\n",{"type":24,"tag":148,"props":1124,"children":1125},{"class":150,"line":187},[1126,1130,1134,1138],{"type":24,"tag":148,"props":1127,"children":1128},{"style":477},[1129],{"type":29,"value":503},{"type":24,"tag":148,"props":1131,"children":1132},{"style":155},[1133],{"type":29,"value":508},{"type":24,"tag":148,"props":1135,"children":1136},{"style":477},[1137],{"type":29,"value":513},{"type":24,"tag":148,"props":1139,"children":1140},{"style":155},[1141],{"type":29,"value":518},{"type":24,"tag":148,"props":1143,"children":1144},{"class":150,"line":208},[1145,1149,1153,1157],{"type":24,"tag":148,"props":1146,"children":1147},{"style":477},[1148],{"type":29,"value":526},{"type":24,"tag":148,"props":1150,"children":1151},{"style":155},[1152],{"type":29,"value":508},{"type":24,"tag":148,"props":1154,"children":1155},{"style":477},[1156],{"type":29,"value":535},{"type":24,"tag":148,"props":1158,"children":1159},{"style":176},[1160],{"type":29,"value":540},{"type":24,"tag":125,"props":1162,"children":1163},{"id":693},[1164],{"type":29,"value":1165},"Scheduler Service",{"type":24,"tag":25,"props":1167,"children":1168},{},[1169],{"type":29,"value":1170},"The re-design of scheduler service is not trivial, the idea is to split workload among multiple instances.\nFor example, fixed max of buckets to 20 and 4 scheduler instances, each scheduler should work on a specific assigned partitions.\nThe first one will handle the first 5 buckets, the second one from the second 5 buckets and so on.",{"type":24,"tag":25,"props":1172,"children":1173},{},[1174],{"type":29,"value":1175},"But what happens when a node fails? Or a new one is scaled-out? In such scenarios, the system requires rebalancing\nto ensure an even distribution of workloads. This calls for a coordination mechanism and cluster\nawareness to dynamically reassign buckets among the available nodes.",{"type":24,"tag":1177,"props":1178,"children":1180},"h4",{"id":1179},"clustering",[1181],{"type":29,"value":1182},"Clustering",{"type":24,"tag":25,"props":1184,"children":1185},{},[1186,1188,1193,1194,1199],{"type":29,"value":1187},"Typically, clusters involve ",{"type":24,"tag":682,"props":1189,"children":1190},{},[1191],{"type":29,"value":1192},"member discovery",{"type":29,"value":960},{"type":24,"tag":682,"props":1195,"children":1196},{},[1197],{"type":29,"value":1198},"failure detection",{"type":29,"value":1200}," to update each instance’s local knowledge about the cluster. By knowing the cluster membership and the fixed bucket size, it becomes possible to assign (or self-assign) a set of partitions. Many techniques and technologies enable cluster formation and facilitate information sharing among members to split up partitions. For example:",{"type":24,"tag":49,"props":1202,"children":1203},{},[1204,1209,1214],{"type":24,"tag":53,"props":1205,"children":1206},{},[1207],{"type":29,"value":1208},"Relying on Leader Election: Using consensus algorithms like Raft or Paxos, a leader is elected to assign partitions.",{"type":24,"tag":53,"props":1210,"children":1211},{},[1212],{"type":29,"value":1213},"Zookeeper: Provides coordination and partition management.",{"type":24,"tag":53,"props":1215,"children":1216},{},[1217],{"type":29,"value":1218},"Distributed Maps: High-level abstractions over consensus algorithms, like Hazelcast, that allow shared state and partition assignment.",{"type":24,"tag":25,"props":1220,"children":1221},{},[1222],{"type":29,"value":1223},"Another alternative - one I will explore and implement in the next article - is to use a cluster membership protocol combined with consistent hashing (hash ring).",{"type":24,"tag":49,"props":1225,"children":1226},{},[1227,1232],{"type":24,"tag":53,"props":1228,"children":1229},{},[1230],{"type":29,"value":1231},"The membership protocol is responsible for detecting members in-out and node failures.",{"type":24,"tag":53,"props":1233,"children":1234},{},[1235],{"type":29,"value":1236},"The hash ring tracks active nodes on a logical ring and enables nodes to self-establish their own partitions. Additionally, the hash ring minimizes the number of partitions re-assignment during a rebalance.",{"type":24,"tag":25,"props":1238,"children":1239},{},[1240,1249],{"type":24,"tag":1241,"props":1242,"children":1246},"a",{"href":1243,"rel":1244},"https://www.toptal.com/big-data/consistent-hashing",[1245],"nofollow",[1247],{"type":29,"value":1248},"Consistent Hashing",{"type":29,"value":1250}," is an elegant solution for balancing workloads in distributed systems and will be a core component of the next design.",{"type":24,"tag":1252,"props":1253,"children":1256},"callout",{"icon":1254,"type":1255},"mdi:warning","warning",[1257,1275],{"type":24,"tag":25,"props":1258,"children":1259},{},[1260,1262,1267,1269,1274],{"type":29,"value":1261},"During scale-out, scale-in, or node failure, cluster members may temporarily see different member counts, leading to ",{"type":24,"tag":682,"props":1263,"children":1264},{},[1265],{"type":29,"value":1266},"partition ownership overlap",{"type":29,"value":1268},". To address this, the scheduler will rely on a persistence layer to acquire a ",{"type":24,"tag":682,"props":1270,"children":1271},{},[1272],{"type":29,"value":1273},"partition lock with fencing",{"type":29,"value":695},{"type":24,"tag":25,"props":1276,"children":1277},{},[1278],{"type":29,"value":1279},"These aspects, along with more details about application-level partitioning, hash rings, and membership protocols, will be deeply covered in the second part of \"Designing a Distributed Scheduler\".",{"type":24,"tag":1177,"props":1281,"children":1283},{"id":1282},"message-bucket-assignment",[1284],{"type":29,"value":1285},"Message bucket assignment",{"type":24,"tag":25,"props":1287,"children":1288},{},[1289],{"type":29,"value":1290},"How do we assign a bucket to a message? It's straightforward: by using a hash function or a round-robin policy.\nFor a hash-based approach, the assignment can be determined as follows:",{"type":24,"tag":137,"props":1292,"children":1294},{"className":601,"code":1293,"language":603,"meta":7,"style":7},"hash(scheduleId) % buckets\n",[1295],{"type":24,"tag":144,"props":1296,"children":1297},{"__ignoreMap":7},[1298],{"type":24,"tag":148,"props":1299,"children":1300},{"class":150,"line":151},[1301,1306,1311,1316],{"type":24,"tag":148,"props":1302,"children":1303},{"style":618},[1304],{"type":29,"value":1305},"hash",{"type":24,"tag":148,"props":1307,"children":1308},{"style":155},[1309],{"type":29,"value":1310},"(scheduleId) ",{"type":24,"tag":148,"props":1312,"children":1313},{"style":477},[1314],{"type":29,"value":1315},"%",{"type":24,"tag":148,"props":1317,"children":1318},{"style":155},[1319],{"type":29,"value":1320}," buckets\n",{"type":24,"tag":25,"props":1322,"children":1323},{},[1324,1326,1331,1333,1338],{"type":29,"value":1325},"Where scheduleId is a unique identifier, such as a UUID.\nWhile the hash function should be well-distributed, like MurMur3 (commonly used in Kafka).\nThis approach ensures that, given a specific ",{"type":24,"tag":144,"props":1327,"children":1329},{"className":1328},[],[1330],{"type":29,"value":556},{"type":29,"value":1332},", the target ",{"type":24,"tag":144,"props":1334,"children":1336},{"className":1335},[],[1337],{"type":29,"value":820},{"type":29,"value":1339}," can always be determined consistently. As a result, updating the message status can be optimized, avoiding scatter-gather queries in MongoDB.",{"type":24,"tag":25,"props":1341,"children":1342},{},[1343],{"type":29,"value":1344},"The system effectively employs two layers of sharding:",{"type":24,"tag":708,"props":1346,"children":1347},{},[1348,1358],{"type":24,"tag":53,"props":1349,"children":1350},{},[1351,1356],{"type":24,"tag":682,"props":1352,"children":1353},{},[1354],{"type":29,"value":1355},"Scheduler-To-Bucket",{"type":29,"value":1357},": Each worker is mapped inside a hash ring and is responsible for its assigned buckets.",{"type":24,"tag":53,"props":1359,"children":1360},{},[1361,1366],{"type":24,"tag":682,"props":1362,"children":1363},{},[1364],{"type":29,"value":1365},"Message-To-Bucket",{"type":29,"value":1367},": Each message is assigned to a specific bucket based on the hash function.",{"type":24,"tag":85,"props":1369,"children":1374},{"src":1370,"alt":1371,"className":1372,"width":1373},"/blog/design-distributed-scheduler-1/images/message-partition-assignment.svg","message-partition-assignment",[90],700,[],{"type":24,"tag":37,"props":1376,"children":1378},{"id":1377},"whats-next",[1379],{"type":29,"value":1380},"What’s Next?",{"type":24,"tag":25,"props":1382,"children":1383},{},[1384],{"type":29,"value":1385},"This article introduced the foundational design for a scalable, distributed scheduler. While we explored key concepts such as bucket-based sharding, partition ownership, and cluster coordination, there’s still more to uncover.",{"type":24,"tag":25,"props":1387,"children":1388},{},[1389],{"type":29,"value":1390},"In the next article, I’ll delve deeper into:",{"type":24,"tag":49,"props":1392,"children":1393},{},[1394,1399,1404],{"type":24,"tag":53,"props":1395,"children":1396},{},[1397],{"type":29,"value":1398},"Cluster membership and Hash Rings: Used to establish partition ownership to self-distribute workloads across nodes.",{"type":24,"tag":53,"props":1400,"children":1401},{},[1402],{"type":29,"value":1403},"Partition Locking with Fencing Tokens: Approach to prevent ownership overlaps during transient states like scale-in, scale-out, or node failures.",{"type":24,"tag":53,"props":1405,"children":1406},{},[1407],{"type":29,"value":1408},"Polling: Optimization to reduce every second polling on database.",{"type":24,"tag":1410,"props":1411,"children":1412},"style",{},[1413],{"type":29,"value":1414},"html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",{"title":7,"searchDepth":161,"depth":161,"links":1416},[1417,1418,1419,1425,1426,1430],{"id":39,"depth":161,"text":42},{"id":75,"depth":161,"text":78},{"id":100,"depth":161,"text":103,"children":1420},[1421,1422,1423,1424],{"id":127,"depth":187,"text":130},{"id":362,"depth":187,"text":365},{"id":448,"depth":187,"text":451},{"id":672,"depth":187,"text":675},{"id":698,"depth":161,"text":701},{"id":773,"depth":161,"text":776,"children":1427},[1428,1429],{"id":807,"depth":187,"text":103},{"id":693,"depth":187,"text":1165},{"id":1377,"depth":161,"text":1380},"markdown","content:blog:design-distributed-scheduler-1:index.md","content","blog/design-distributed-scheduler-1/index.md","blog/design-distributed-scheduler-1/index","md",1733957528820]