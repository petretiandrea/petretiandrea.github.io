<!DOCTYPE html><html lang="it"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><meta name="generator" content="Astro v5.15.9"><title>Design Distributed Scheduler - I</title><meta name="description" content="This article explores the design of a distributed scheduler optimized for scalability and fault tolerance. It covers key challenges like partitioning tasks, managing concurrency, and ensuring load distribution across multiple instances."><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://petretiandrea.github.io/blog/design-distributed-scheduler-1/"><meta property="og:title" content="Design Distributed Scheduler - I"><meta property="og:description" content="This article explores the design of a distributed scheduler optimized for scalability and fault tolerance. It covers key challenges like partitioning tasks, managing concurrency, and ensuring load distribution across multiple instances."><meta property="og:image" content="https://petretiandrea.github.io/blog/design-distributed-scheduler-1/images/thumbnail.webp"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://petretiandrea.github.io/blog/design-distributed-scheduler-1/"><meta property="twitter:title" content="Design Distributed Scheduler - I"><meta property="twitter:description" content="This article explores the design of a distributed scheduler optimized for scalability and fault tolerance. It covers key challenges like partitioning tasks, managing concurrency, and ensuring load distribution across multiple instances."><meta property="twitter:image" content="https://petretiandrea.github.io/blog/design-distributed-scheduler-1/images/thumbnail.webp"><link rel="stylesheet" href="/_astro/admin.BZe_FqMB.css">
<style>.toc[data-astro-cid-xvrfupwn]{max-height:calc(100vh - 6rem);overflow-y:auto}.toc[data-astro-cid-xvrfupwn]::-webkit-scrollbar{width:4px}.toc[data-astro-cid-xvrfupwn]::-webkit-scrollbar-track{background:transparent}.toc[data-astro-cid-xvrfupwn]::-webkit-scrollbar-thumb{background:#cbd5e0;border-radius:4px}.toc[data-astro-cid-xvrfupwn]::-webkit-scrollbar-thumb:hover{background:#a0aec0}
</style>
<link rel="stylesheet" href="/_astro/_slug_.Q4yo_7z7.css"></head> <body> <div class="dark:bg-gray-900 bg-white"> <section class="w-full px-6 pb-12 antialiased min-h-screen"> <div class="mx-auto max-w-8xl min-h-screen flex flex-col"> <nav class="sticky top-0 z-50 w-full bg-white dark:bg-gray-900 border-b dark:border-gray-700 border-gray-200"> <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"> <div class="flex items-center justify-between h-16"> <!-- Menu items --> <div class="flex items-center space-x-1"> <a href="/" class="px-4 py-2 rounded-md text-base font-semibold transition-colors duration-200 hover:bg-gray-100 dark:hover:bg-gray-800 text-gray-700 dark:text-gray-200"> Home </a><a href="/projects" class="px-4 py-2 rounded-md text-base font-semibold transition-colors duration-200 hover:bg-gray-100 dark:hover:bg-gray-800 text-gray-700 dark:text-gray-200"> Progetti </a><a href="/blog" class="px-4 py-2 rounded-md text-base font-semibold transition-colors duration-200 hover:bg-gray-100 dark:hover:bg-gray-800 text-indigo-600 dark:text-indigo-400 bg-gray-50 dark:bg-gray-800"> Blog </a> </div> <!-- Language switcher (hidden on blog pages since blog is Italian only) -->  </div> </div> </nav> <main class="flex-1">   <div class="min-h-screen bg-white dark:bg-gray-900"> <!-- Hero section --> <article class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-12"> <div class="max-w-3xl mx-auto"> <!-- Title --> <h1 class="text-4xl sm:text-5xl font-bold text-gray-900 dark:text-gray-100 mb-6 leading-tight"> Design Distributed Scheduler - I </h1> <!-- Subtitle/Description --> <p class="text-xl text-gray-600 dark:text-gray-400 mb-8"> This article explores the design of a distributed scheduler optimized for scalability and fault tolerance. It covers key challenges like partitioning tasks, managing concurrency, and ensuring load distribution across multiple instances. </p> <!-- Author info --> <div class="flex items-center justify-between mb-8 pb-8 border-b border-gray-200 dark:border-gray-700"> <div class="flex items-center space-x-4"> <img src="/img/author2.jpg" alt="Andrea Petreti" loading="eager" decoding="async" fetchpriority="auto" width="48" height="48" class="w-12 h-12 rounded-full object-cover"> <div> <div class="font-medium text-gray-900 dark:text-gray-100"> Andrea Petreti </div> <div class="flex items-center space-x-2 text-sm text-gray-600 dark:text-gray-400"> <time>December 12, 2024</time> <span>·</span> <span>11 min read</span> </div> </div> </div> <!-- Tags --> <div class="hidden sm:flex flex-wrap gap-2"> <span class="text-xs bg-gray-100 dark:bg-gray-800 text-gray-700 dark:text-gray-300 px-3 py-1 rounded-full"> shard </span><span class="text-xs bg-gray-100 dark:bg-gray-800 text-gray-700 dark:text-gray-300 px-3 py-1 rounded-full"> partitions </span><span class="text-xs bg-gray-100 dark:bg-gray-800 text-gray-700 dark:text-gray-300 px-3 py-1 rounded-full"> scheduler </span> </div> </div> <!-- Featured Image - Widescreen (optional) --> <div class="w-full aspect-[21/9] rounded-lg mb-12 shadow-2xl overflow-hidden bg-gradient-to-br from-indigo-500 to-purple-600"> <img src="/blog/design-distributed-scheduler-1/images/thumbnail.webp" alt="Design Distributed Scheduler - I" loading="eager" decoding="async" fetchpriority="auto" width="1400" height="600" class="w-full h-full object-cover"> </div> </div> <!-- Content with TOC --> <div class="max-w-7xl mx-auto"> <div class="lg:grid lg:grid-cols-12 lg:gap-8"> <!-- Main content --> <div class="lg:col-span-8"> <div class="prose prose-lg dark:prose-invert prose-indigo max-w-none
                        prose-headings:font-bold prose-headings:text-gray-900 dark:prose-headings:text-gray-100
                        prose-p:text-gray-700 dark:prose-p:text-gray-300
                        prose-a:text-indigo-600 dark:prose-a:text-indigo-400
                        prose-strong:text-gray-900 dark:prose-strong:text-gray-100
                        prose-code:text-indigo-600 dark:prose-code:text-indigo-400
                        prose-pre:bg-gray-900 dark:prose-pre:bg-gray-950
                        prose-img:rounded-lg"> <p>Having a distributed scheduler can be useful for orchestration purposes. For instance, it can be used to send a reminder after X minutes or enforce a workflow timeout (e.g., ensuring a payment is completed within Y minutes). However, designing a scheduler in a distributed environment is no trivial task, especially when aiming for high availability and scalability. This article explores the challenges and strategies for designing an efficient distributed scheduler.</p>
<p>In this design, I will focus primarily on the challenges of achieving scalability and high availability. To keep things straightforward, I will simplify the scheduler’s requirements. For example, it will not handle retries for failed jobs, nor will it deal with job-specific concepts. Instead, the scheduler will operate on an abstract level with messages or commands. These messages or commands can be executed by any worker, potentially even the same worker that requested the scheduling.</p>
<h2 id="use-cases">Use cases</h2>
<p>For this system let’s assume the following use cases:</p>
<ul>
<li>user can schedule a message to be executed at given time</li>
<li>user can cancel a scheduled message</li>
<li>user can specify a channel (something similar to a kafka topic) where they want to receive the message</li>
</ul>
<p>Essentially, the system will function like a delayed queue, where messages are delivered at a specified time. When a message’s time is up, the system will publish it to a real queue (e.g., Kafka, RabbitMQ).</p>
<h2 id="big-picture-architecture">Big picture architecture</h2>
<p>The main idea is to create a scheduler service which reads pending messages at given time, submit to queue and then track it’s progress, like an offset. When a message is successfully delivered to destination topic then the scheduler update the message status (ack/nack).</p>
<img src="images/big-picture.webp" alt="big-picture" class="center-md-image" width="500">
<p>Event the diagram shows multiple instance of scheduler service, this design start by
exploring a solution based on single instance Scheduler and then starts by making some re-design
to make system horizontally scalable.</p>
<h2 id="database">Database</h2>
<p>A robust database schema is a crucial element that directly impacts overall system performance. To optimize efficiency, we can start by splitting the read and write loads based on the system’s use cases.</p>
<p>It’s easy to see that the primary type of load on the database is “read”. The scheduler will periodically (e.g., every X minutes) query the pending scheduled messages to determine which ones need to be submitted immediately.</p>
<p>What about the write load? From the perspective of external actors, the system supports creating new scheduled messages or deleting existing ones. Internally, the scheduler also needs to update the status of messages once they are completed or have failed.</p>
<p>The key challenge is to design an efficient way to retrieve messages that are ready to be submitted. For this design, I’ll use MongoDB, a NoSQL database that enables easy scaling and partitioning of data.</p>
<h3 id="schema-design">Schema design</h3>
<p>A database schema for NoSQL like database</p>
<pre class="astro-code github-dark-dimmed" style="background-color:#22272e;color:#adbac7; overflow-x: auto;" tabindex="0" data-language="json"><code><span class="line"><span style="color:#ADBAC7">{</span></span>
<span class="line"><span style="color:#8DDB8C">	"scheduleId"</span><span style="color:#ADBAC7">: </span><span style="color:#96D0FF">"..."</span><span style="color:#ADBAC7">,</span></span>
<span class="line"><span style="color:#8DDB8C">	"destinationTopic"</span><span style="color:#ADBAC7">: </span><span style="color:#96D0FF">"..."</span><span style="color:#ADBAC7">,</span></span>
<span class="line"><span style="color:#8DDB8C">	"executionTime"</span><span style="color:#ADBAC7">: </span><span style="color:#96D0FF">"date-time"</span><span style="color:#ADBAC7">,</span></span>
<span class="line"><span style="color:#8DDB8C">	"payload"</span><span style="color:#ADBAC7">: </span><span style="color:#96D0FF">"scheduled command payload"</span><span style="color:#ADBAC7">,</span></span>
<span class="line"><span style="color:#8DDB8C">	"status"</span><span style="color:#ADBAC7">: </span><span style="color:#96D0FF">"PENDING|DELIVERED|DELIVERY_FAILED"</span></span>
<span class="line"><span style="color:#ADBAC7">}</span></span></code></pre>
<p>The schema is pretty simple with few fields:</p>
<ul>
<li><code>payload</code>: Contains the data for the scheduled job. This payload will be delivered to the queue.</li>
<li><code>status</code>: Tracks the schedule status. The default status is <code>PENDING</code>, while <code>DELIVERED</code> means the message has been sent to the <code>destinationTopic</code>, and <code>DELIVERY_FAILED</code> indicates a failure to deliver the message to the queue.</li>
<li><code>executionTime</code>: A timestamp, rounded to the nearest minute or second, based on the scheduler’s granularity.</li>
<li><code>destinationTopic</code>: Specifies the topic where the scheduler will publish the message. This could be a Kafka topic or a RabbitMQ routing key.</li>
</ul>
<h3 id="storing-offset-and-execution-time">Storing Offset and Execution Time</h3>
<p>In addition to the main scheduler collection, the system will have a dedicated collection to store the offset and the last <code>executionTime</code>. This will allow the scheduler to track the progress of scheduled messages more effectively.
The structure of this collection could look like this:</p>
<pre class="astro-code github-dark-dimmed" style="background-color:#22272e;color:#adbac7; overflow-x: auto;" tabindex="0" data-language="json"><code><span class="line"><span style="color:#ADBAC7">{</span></span>
<span class="line"><span style="color:#8DDB8C">	"lastExecutionTime"</span><span style="color:#ADBAC7">: </span><span style="color:#96D0FF">"date-time"</span></span>
<span class="line"><span style="color:#ADBAC7">}</span></span></code></pre>
<p>Where:</p>
<ul>
<li><code>lastTaskId</code>: A unique identifier for the task being tracked.</li>
<li><code>lastExecutionTime</code>: The last time the task was executed, helping to track the progress and avoid redundant processing.</li>
</ul>
<h3 id="optimize-queries">Optimize queries</h3>
<p>The most important aspect is query optimization, as these will be performed periodically by the scheduler. The challenge is to efficiently retrieve the next messages to be sent. Example of SQL-like query to retrieve messages for next execution time. e.g. current <code>executionTime > 2024/09/25T12:50:00Z AND executionTime &#x3C; 2024/09/25T12:50:00Z</code></p>
<pre class="astro-code github-dark-dimmed" style="background-color:#22272e;color:#adbac7; overflow-x: auto;" tabindex="0" data-language="sql"><code><span class="line"><span style="color:#F47067">SELECT</span><span style="color:#F47067"> *</span><span style="color:#F47067"> FROM</span><span style="color:#ADBAC7"> schedules </span></span>
<span class="line"><span style="color:#F47067">WHERE</span><span style="color:#ADBAC7"> executionTime </span><span style="color:#F47067">></span><span style="color:#ADBAC7"> {lastExecutionTime} </span></span>
<span class="line"><span style="color:#F47067">AND</span><span style="color:#ADBAC7"> executionTime </span><span style="color:#F47067">&#x3C;</span><span style="color:#96D0FF"> "2024/09/25T12:50:01Z"</span><span style="color:#ADBAC7"> </span></span></code></pre>
<p>Why use a range query? It makes handling issues related to timestamp precision or small variations in service processing easier, especially in failure scenarios.</p>
<p>Without a proper data partition strategy, this query could result in a full scan of all shards. For example, if using <code>scheduleId</code> as the shard key, running this query would span multiple partitions. To optimize the search query, a better shard key would be <code>executionTime</code>. This approach allows the query to be answered by exploring a single shard or a reduced number of shards. In MongoDB, this type of shard key is called a Range Shard Key. MongoDB automatically splits the data space into multiple ranges and redirects the request to the most appropriate shard or set of shards. From an abstract point of view, it’s as if we are grouping all jobs with the same scheduling date into the same group.</p>
<p>The second operation involves updating the message status to either <code>DELIVERED</code> or <code>DELIVERY_FAILED</code>.
This operation will use <code>scheduleId</code> as the unique identifier. However, if we only use <code>executionTime</code> as the shard key,
the write operation may lead to a scatter-gather query, which can reduce efficiency.
To improve performance, we can create a composite shard key by combining two keys.
This allows for more efficient updates by directing the operation to a specific shard.</p>
<pre class="astro-code github-dark-dimmed" style="background-color:#22272e;color:#adbac7; overflow-x: auto;" tabindex="0" data-language="js"><code><span class="line"><span style="color:#ADBAC7">db.</span><span style="color:#DCBDFB">shardCollection</span><span style="color:#ADBAC7">(</span><span style="color:#96D0FF">"messages"</span><span style="color:#ADBAC7">, {executionTime: </span><span style="color:#6CB6FF">1</span><span style="color:#ADBAC7">, scheduleId: </span><span style="color:#6CB6FF">1</span><span style="color:#ADBAC7">})</span></span></code></pre>
<p>With this composite shard key, the MongoDB query planner will target a single shard for each operation,
improving the performance and efficiency of writes. The first state in the query planner would be <code>SINGLE_SHARD</code>,
meaning the query no longer needs to perform a scatter-gather across multiple shards.</p>
<h3 id="dealing-with-skewed-partitions-and-hotspotting">Dealing with Skewed Partitions and Hotspotting</h3>
<p>What happens if many jobs are scheduled at the same time?
It depends on the granularity of the executionTime (e.g., rounding the timestamp to the second or minute),
but it could lead to <strong>skewed partitions</strong>.
MongoDB also faces issues with Monotonic Increasing Keys, like timestamps, which can cause shard hotspotting.
This happens when new data constantly gets routed to the same shard due to a sequence of timestamps being close to each other.
Over time, this leads to uneven load distribution and potential performance bottlenecks.
We will address how to resolve this issue in the next section when discussing the <strong>scheduler-service</strong>.</p>
<h2 id="design-scheduler-service">Design Scheduler service</h2>
<p>In this first scheduler design, it’s behaviour is quite  simple.</p>
<ol>
<li>polling every minute/second for messages with <code>scheduledTime</code> after last offeset</li>
<li>check if message is <code>PENDING</code></li>
<li>send a message to queue with payload</li>
<li>update job status based on queue’s publish outcome</li>
<li>checkpoint the offset by updating document.</li>
</ol>
<p>Optionally: To improve performance and reduce writes to the database, checkpointing can be performed every X seconds.
The downside of this approach is that the service could crash without writing its last checkpoint.
When the service restarts, it will re-read the previous batch of messages,
but it won’t produce duplicate messages due to the message status field.</p>
<img src="images/sequence.svg" alt="sequence" class="center-md-image" width="600">
<p>Obviously, the polling frequency determines the scheduler’s granularity.
For now, let’s assume second-level granularity, even though this implies a lot of queries to the database.
In subsequent designs, I will explore ways to improve the system and reduce unnecessary queries
(spoiler: using an actor-like approach and caching).</p>
<p>The system seems ready for implementation, but what about horizontal scalability?
If I want to run multiple instances of the scheduler, this could lead to concurrency issues.
For example, two different instances might try to poll the same jobs and send the same events.
Additionally, there is no load distribution between instances!</p>
<h2 id="re-design-make-it-scalable">Re-Design: Make it scalable!</h2>
<p>Let’s make a little bit of redesign to address some issues:</p>
<ul>
<li>High load for single worker when there are a lot of scheduled job at same second</li>
<li>The scheduler service cannot properly scale horizontally. Increasing instances doesn’t distribute the load.</li>
<li>The scheduler service is a single point of failure.</li>
</ul>
<p>The goal is to distribute the load at the application level. For example, if you have 10,000 jobs scheduled at the same time, we can split them into 10,000 / number of instances.
This way, each service will handle a different partition and poll only for its assigned partitions.</p>
<h3 id="database-1">Database</h3>
<p>At the database schema level, we need to introduce a new field: <code>bucket</code>.
Each bucket can contain multiple scheduled messages at the same second. Suppose we define a total number of buckets (e.g. 20),
this means that messages scheduled at the same time can potentially be processed by a maximum of <strong>20 parallel instances</strong>.
So, the number of buckets is a hyperparameter that should be fine-tuned based on load expectations, something
similar to the number of partitions in a Kafka topic.</p>
<p>So extend the schema in the following way:</p>
<pre class="astro-code github-dark-dimmed" style="background-color:#22272e;color:#adbac7; overflow-x: auto;" tabindex="0" data-language="json"><code><span class="line"><span style="color:#ADBAC7">{ </span></span>
<span class="line"><span style="color:#8DDB8C">	"scheduleId"</span><span style="color:#ADBAC7">: </span><span style="color:#96D0FF">"..."</span><span style="color:#ADBAC7">,</span></span>
<span class="line"><span style="color:#8DDB8C">	"executionTime"</span><span style="color:#ADBAC7">: </span><span style="color:#96D0FF">"date-time"</span><span style="color:#ADBAC7">,</span></span>
<span class="line"><span style="color:#8DDB8C">	"bucket"</span><span style="color:#ADBAC7">: </span><span style="color:#6CB6FF">1</span><span style="color:#ADBAC7">,</span></span>
<span class="line"><span style="color:#8DDB8C">	"payload"</span><span style="color:#ADBAC7">: </span><span style="color:#96D0FF">"scheduled command payload"</span><span style="color:#ADBAC7">,</span></span>
<span class="line"><span style="color:#8DDB8C">	"status"</span><span style="color:#ADBAC7">: </span><span style="color:#96D0FF">"PENDING|DELIVERED|DELIVERY_FAILED"</span></span>
<span class="line"><span style="color:#ADBAC7">}</span></span></code></pre>
<p>We can leverage the combination of <code>executionTime</code> and <code>bucket</code> as a composite shard key to improve and parallelize
reads across multiple instances. Messages scheduled at the same time and in the same bucket will be colocated
in the same MongoDB partition, which helps reduce partition skew for jobs scheduled at identical times.</p>
<p>However, the MongoDB monotonic timestamp issue still persists. To mitigate this,
using <code>bucket</code> as the shard key prefix (the first part of the composite key) can help reduce the negative effects
of a monotonic timestamp:</p>
<pre class="astro-code github-dark-dimmed" style="background-color:#22272e;color:#adbac7; overflow-x: auto;" tabindex="0" data-language="js"><code><span class="line"><span style="color:#ADBAC7">db.</span><span style="color:#DCBDFB">shardCollection</span><span style="color:#ADBAC7">(</span><span style="color:#96D0FF">"messages"</span><span style="color:#ADBAC7">, {bucket: </span><span style="color:#96D0FF">"hashed"</span><span style="color:#ADBAC7">, executionTime: </span><span style="color:#6CB6FF">1</span><span style="color:#ADBAC7">, scheduleId: </span><span style="color:#6CB6FF">1</span><span style="color:#ADBAC7">})</span></span></code></pre>
<p>Even though <code>bucket</code> is a low-cardinality key, its combination with <code>executionTime</code> (which is a high-cardinality and monotonic key)
and <code>scheduleId</code> (also a high-cardinality key) gives MongoDB enough information to effectively split the shards without issues. Additionally,
using a “hashed” <code>bucket</code> ensures event data distribution across shards, thereby avoiding hotspotting.
Now the query to retrieve next scheduled messages looks like the following one:</p>
<pre class="astro-code github-dark-dimmed" style="background-color:#22272e;color:#adbac7; overflow-x: auto;" tabindex="0" data-language="sql"><code><span class="line"><span style="color:#F47067">SELECT</span><span style="color:#F47067"> *</span><span style="color:#F47067"> FROM</span><span style="color:#ADBAC7"> schedules </span></span>
<span class="line"><span style="color:#F47067">WHERE</span><span style="color:#ADBAC7"> bucket </span><span style="color:#F47067">=</span><span style="color:#ADBAC7"> {schedulerInstanceAssignedBucket} </span><span style="color:#F47067">AND</span></span>
<span class="line"><span style="color:#F47067">WHERE</span><span style="color:#ADBAC7"> executionTime </span><span style="color:#F47067">></span><span style="color:#ADBAC7"> {lastExecutionTime} </span></span>
<span class="line"><span style="color:#F47067">AND</span><span style="color:#ADBAC7"> executionTime </span><span style="color:#F47067">&#x3C;</span><span style="color:#96D0FF"> "2024/09/25T12:50:01Z"</span></span></code></pre>
<h3 id="scheduler-service">Scheduler Service</h3>
<p>The re-design of scheduler service is not trivial, the idea is to split workload among multiple instances.
For example, fixed max of buckets to 20 and 4 scheduler instances, each scheduler should work on a specific assigned partitions.
The first one will handle the first 5 buckets, the second one from the second 5 buckets and so on.</p>
<p>But what happens when a node fails? Or a new one is scaled-out? In such scenarios, the system requires rebalancing
to ensure an even distribution of workloads. This calls for a coordination mechanism and cluster
awareness to dynamically reassign buckets among the available nodes.</p>
<h4 id="clustering">Clustering</h4>
<p>Typically, clusters involve <strong>member discovery</strong> and <strong>failure detection</strong> to update each instance’s local knowledge about the cluster. By knowing the cluster membership and the fixed bucket size, it becomes possible to assign (or self-assign) a set of partitions. Many techniques and technologies enable cluster formation and facilitate information sharing among members to split up partitions. For example:</p>
<ul>
<li>Relying on Leader Election: Using consensus algorithms like Raft or Paxos, a leader is elected to assign partitions.</li>
<li>Zookeeper: Provides coordination and partition management.</li>
<li>Distributed Maps: High-level abstractions over consensus algorithms, like Hazelcast, that allow shared state and partition assignment.</li>
</ul>
<p>Another alternative - one I will explore and implement in the next article - is to use a cluster membership protocol combined with consistent hashing (hash ring).</p>
<ul>
<li>The membership protocol is responsible for detecting members in-out and node failures.</li>
<li>The hash ring tracks active nodes on a logical ring and enables nodes to self-establish their own partitions. Additionally, the hash ring minimizes the number of partitions re-assignment during a rebalance.</li>
</ul>
<p><a href="https://www.toptal.com/big-data/consistent-hashing">Consistent Hashing</a> is an elegant solution for balancing workloads in distributed systems and will be a core component of the next design.</p>
<div class="callout callout-warning" data-callout="warning"><p>During scale-out, scale-in, or node failure, cluster members may temporarily see different member counts, leading to <strong>partition ownership overlap</strong>. To address this, the scheduler will rely on a persistence layer to acquire a <strong>partition lock with fencing</strong>.</p><p>These aspects, along with more details about application-level partitioning, hash rings, and membership protocols, will be deeply covered in the second part of “Designing a Distributed Scheduler”.</p></div>
<h4 id="message-bucket-assignment">Message bucket assignment</h4>
<p>How do we assign a bucket to a message? It’s straightforward: by using a hash function or a round-robin policy.
For a hash-based approach, the assignment can be determined as follows:</p>
<pre class="astro-code github-dark-dimmed" style="background-color:#22272e;color:#adbac7; overflow-x: auto;" tabindex="0" data-language="js"><code><span class="line"><span style="color:#DCBDFB">hash</span><span style="color:#ADBAC7">(scheduleId) </span><span style="color:#F47067">%</span><span style="color:#ADBAC7"> buckets</span></span></code></pre>
<p>Where scheduleId is a unique identifier, such as a UUID.
While the hash function should be well-distributed, like MurMur3 (commonly used in Kafka).
This approach ensures that, given a specific <code>scheduleId</code>, the target <code>bucket</code> can always be determined consistently. As a result, updating the message status can be optimized, avoiding scatter-gather queries in MongoDB.</p>
<p>The system effectively employs two layers of sharding:</p>
<ol>
<li><strong>Scheduler-To-Bucket</strong>: Each worker is mapped inside a hash ring and is responsible for its assigned buckets.</li>
<li><strong>Message-To-Bucket</strong>: Each message is assigned to a specific bucket based on the hash function.</li>
</ol>
<img src="images/message-partition-assignment.svg" alt="message-partition-assignment" class="center-md-image" width="700">
<h2 id="whats-next">What’s Next?</h2>
<p>This article introduced the foundational design for a scalable, distributed scheduler. While we explored key concepts such as bucket-based sharding, partition ownership, and cluster coordination, there’s still more to uncover.</p>
<p>In the next article, I’ll delve deeper into:</p>
<ul>
<li>Cluster membership and Hash Rings: Used to establish partition ownership to self-distribute workloads across nodes.</li>
<li>Partition Locking with Fencing Tokens: Approach to prevent ownership overlaps during transient states like scale-in, scale-out, or node failures.</li>
<li>Polling: Optimization to reduce every second polling on database.</li>
</ul> </div> <!-- Comments Section --> <div class="mt-16 pt-16 border-t border-gray-200 dark:border-gray-700"> <h2 class="text-2xl font-bold text-gray-900 dark:text-gray-100 mb-8">Comments</h2> <style>astro-island,astro-slot,astro-static-slot{display:contents}</style><script>(()=>{var e=async t=>{await(await t())()};(self.Astro||(self.Astro={})).only=e;window.dispatchEvent(new Event("astro:only"));})();</script><script>(()=>{var A=Object.defineProperty;var g=(i,o,a)=>o in i?A(i,o,{enumerable:!0,configurable:!0,writable:!0,value:a}):i[o]=a;var d=(i,o,a)=>g(i,typeof o!="symbol"?o+"":o,a);{let i={0:t=>m(t),1:t=>a(t),2:t=>new RegExp(t),3:t=>new Date(t),4:t=>new Map(a(t)),5:t=>new Set(a(t)),6:t=>BigInt(t),7:t=>new URL(t),8:t=>new Uint8Array(t),9:t=>new Uint16Array(t),10:t=>new Uint32Array(t),11:t=>1/0*t},o=t=>{let[l,e]=t;return l in i?i[l](e):void 0},a=t=>t.map(o),m=t=>typeof t!="object"||t===null?t:Object.fromEntries(Object.entries(t).map(([l,e])=>[l,o(e)]));class y extends HTMLElement{constructor(){super(...arguments);d(this,"Component");d(this,"hydrator");d(this,"hydrate",async()=>{var b;if(!this.hydrator||!this.isConnected)return;let e=(b=this.parentElement)==null?void 0:b.closest("astro-island[ssr]");if(e){e.addEventListener("astro:hydrate",this.hydrate,{once:!0});return}let c=this.querySelectorAll("astro-slot"),n={},h=this.querySelectorAll("template[data-astro-template]");for(let r of h){let s=r.closest(this.tagName);s!=null&&s.isSameNode(this)&&(n[r.getAttribute("data-astro-template")||"default"]=r.innerHTML,r.remove())}for(let r of c){let s=r.closest(this.tagName);s!=null&&s.isSameNode(this)&&(n[r.getAttribute("name")||"default"]=r.innerHTML)}let p;try{p=this.hasAttribute("props")?m(JSON.parse(this.getAttribute("props"))):{}}catch(r){let s=this.getAttribute("component-url")||"<unknown>",v=this.getAttribute("component-export");throw v&&(s+=` (export ${v})`),console.error(`[hydrate] Error parsing props for component ${s}`,this.getAttribute("props"),r),r}let u;await this.hydrator(this)(this.Component,p,n,{client:this.getAttribute("client")}),this.removeAttribute("ssr"),this.dispatchEvent(new CustomEvent("astro:hydrate"))});d(this,"unmount",()=>{this.isConnected||this.dispatchEvent(new CustomEvent("astro:unmount"))})}disconnectedCallback(){document.removeEventListener("astro:after-swap",this.unmount),document.addEventListener("astro:after-swap",this.unmount,{once:!0})}connectedCallback(){if(!this.hasAttribute("await-children")||document.readyState==="interactive"||document.readyState==="complete")this.childrenConnectedCallback();else{let e=()=>{document.removeEventListener("DOMContentLoaded",e),c.disconnect(),this.childrenConnectedCallback()},c=new MutationObserver(()=>{var n;((n=this.lastChild)==null?void 0:n.nodeType)===Node.COMMENT_NODE&&this.lastChild.nodeValue==="astro:end"&&(this.lastChild.remove(),e())});c.observe(this,{childList:!0}),document.addEventListener("DOMContentLoaded",e)}}async childrenConnectedCallback(){let e=this.getAttribute("before-hydration-url");e&&await import(e),this.start()}async start(){let e=JSON.parse(this.getAttribute("opts")),c=this.getAttribute("client");if(Astro[c]===void 0){window.addEventListener(`astro:${c}`,()=>this.start(),{once:!0});return}try{await Astro[c](async()=>{let n=this.getAttribute("renderer-url"),[h,{default:p}]=await Promise.all([import(this.getAttribute("component-url")),n?import(n):()=>()=>{}]),u=this.getAttribute("component-export")||"default";if(!u.includes("."))this.Component=h[u];else{this.Component=h;for(let f of u.split("."))this.Component=this.Component[f]}return this.hydrator=p,this.hydrate},e,this)}catch(n){console.error(`[astro-island] Error hydrating ${this.getAttribute("component-url")}`,n)}}attributeChangedCallback(){this.hydrate()}}d(y,"observedAttributes",["props"]),customElements.get("astro-island")||customElements.define("astro-island",y)}})();</script><astro-island uid="22y2VB" component-url="/_astro/GiscusComments.lkjKpGS6.js" component-export="default" renderer-url="/_astro/client.BLUn-lwI.js" props="{&quot;repo&quot;:[0,&quot;petretiandrea/petretiandrea.github.io&quot;],&quot;repoId&quot;:[0,&quot;MDEwOlJlcG9zaXRvcnkyODQyMjkyOTc=&quot;],&quot;category&quot;:[0,&quot;Q&amp;A&quot;],&quot;categoryId&quot;:[0,&quot;DIC_kwDOEPD-sc4ClIwk&quot;],&quot;term&quot;:[0,&quot;design-distributed-scheduler-1/index.md&quot;]}" ssr client="only" opts="{&quot;name&quot;:&quot;GiscusComments&quot;,&quot;value&quot;:&quot;react&quot;}"></astro-island> </div> </div> <!-- TOC Sidebar --> <aside class="hidden lg:block lg:col-span-4"> <nav class="toc sticky top-20" data-astro-cid-xvrfupwn><div class="bg-gray-50 dark:bg-gray-800 rounded-lg p-6 border border-gray-200 dark:border-gray-700" data-astro-cid-xvrfupwn><h2 class="text-sm font-semibold text-gray-900 dark:text-gray-100 uppercase tracking-wider mb-4" data-astro-cid-xvrfupwn>
Table of Contents
</h2><ul class="space-y-2 text-sm" data-astro-cid-xvrfupwn><li style="margin-left: 0rem" data-astro-cid-xvrfupwn><a href="#use-cases" class="block text-gray-600 dark:text-gray-400 hover:text-indigo-600 dark:hover:text-indigo-400 transition-colors duration-200 py-1" data-astro-cid-xvrfupwn>Use cases</a></li><li style="margin-left: 0rem" data-astro-cid-xvrfupwn><a href="#big-picture-architecture" class="block text-gray-600 dark:text-gray-400 hover:text-indigo-600 dark:hover:text-indigo-400 transition-colors duration-200 py-1" data-astro-cid-xvrfupwn>Big picture architecture</a></li><li style="margin-left: 0rem" data-astro-cid-xvrfupwn><a href="#database" class="block text-gray-600 dark:text-gray-400 hover:text-indigo-600 dark:hover:text-indigo-400 transition-colors duration-200 py-1" data-astro-cid-xvrfupwn>Database</a></li><li style="margin-left: 1rem" data-astro-cid-xvrfupwn><a href="#schema-design" class="block text-gray-600 dark:text-gray-400 hover:text-indigo-600 dark:hover:text-indigo-400 transition-colors duration-200 py-1" data-astro-cid-xvrfupwn>Schema design</a></li><li style="margin-left: 1rem" data-astro-cid-xvrfupwn><a href="#storing-offset-and-execution-time" class="block text-gray-600 dark:text-gray-400 hover:text-indigo-600 dark:hover:text-indigo-400 transition-colors duration-200 py-1" data-astro-cid-xvrfupwn>Storing Offset and Execution Time</a></li><li style="margin-left: 1rem" data-astro-cid-xvrfupwn><a href="#optimize-queries" class="block text-gray-600 dark:text-gray-400 hover:text-indigo-600 dark:hover:text-indigo-400 transition-colors duration-200 py-1" data-astro-cid-xvrfupwn>Optimize queries</a></li><li style="margin-left: 1rem" data-astro-cid-xvrfupwn><a href="#dealing-with-skewed-partitions-and-hotspotting" class="block text-gray-600 dark:text-gray-400 hover:text-indigo-600 dark:hover:text-indigo-400 transition-colors duration-200 py-1" data-astro-cid-xvrfupwn>Dealing with Skewed Partitions and Hotspotting</a></li><li style="margin-left: 0rem" data-astro-cid-xvrfupwn><a href="#design-scheduler-service" class="block text-gray-600 dark:text-gray-400 hover:text-indigo-600 dark:hover:text-indigo-400 transition-colors duration-200 py-1" data-astro-cid-xvrfupwn>Design Scheduler service</a></li><li style="margin-left: 0rem" data-astro-cid-xvrfupwn><a href="#re-design-make-it-scalable" class="block text-gray-600 dark:text-gray-400 hover:text-indigo-600 dark:hover:text-indigo-400 transition-colors duration-200 py-1" data-astro-cid-xvrfupwn>Re-Design: Make it scalable!</a></li><li style="margin-left: 1rem" data-astro-cid-xvrfupwn><a href="#database-1" class="block text-gray-600 dark:text-gray-400 hover:text-indigo-600 dark:hover:text-indigo-400 transition-colors duration-200 py-1" data-astro-cid-xvrfupwn>Database</a></li><li style="margin-left: 1rem" data-astro-cid-xvrfupwn><a href="#scheduler-service" class="block text-gray-600 dark:text-gray-400 hover:text-indigo-600 dark:hover:text-indigo-400 transition-colors duration-200 py-1" data-astro-cid-xvrfupwn>Scheduler Service</a></li><li style="margin-left: 0rem" data-astro-cid-xvrfupwn><a href="#whats-next" class="block text-gray-600 dark:text-gray-400 hover:text-indigo-600 dark:hover:text-indigo-400 transition-colors duration-200 py-1" data-astro-cid-xvrfupwn>What’s Next?</a></li></ul></div></nav> </aside> </div> </div> </article> </div>  </main> <footer> <div class="flex flex-col items-center pt-16 dark:bg-gray-900"> <div class="flex mb-3 space-x-4"> <a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://github.com/petretiandrea"> <span class="sr-only">github</span> <svg class="w-8 h-8" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true"> <path fill-rule="evenodd" d="M12 2C6.477 2 2 6.484 2 12.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483 0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.026A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.202 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688 0 3.848-2.339 4.695-4.566 4.943.359.309.678.92.678 1.855 0 1.338-.012 2.419-.012 2.747 0 .268.18.58.688.482A10.019 10.019 0 0022 12.017C22 6.484 17.522 2 12 2z" clip-rule="evenodd"></path> </svg> </a> <a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="#"> <span class="sr-only">Linkedin</span> <svg class="w-8 h-8" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true"> <path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"></path> </svg> </a> <a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="mailto:petretiandrea@gmail.com"> <span class="sr-only">mail</span> <svg class="w-8 h-8" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true"> <path d="M0 3v18h24v-18h-24zm21.518 2l-9.518 7.713-9.518-7.713h19.036zm-19.518 14v-11.817l10 8.104 10-8.104v11.817h-20z"></path> </svg> </a> </div> <div class="flex mb-2 space-x-2 text-sm text-gray-500 dark:text-gray-400"> <div>Copyright © 2025</div> <div>•</div> <a href="/"> Andrea Petreti's blog - Love coding </a> </div> <div class="mb-8 text-sm text-gray-500 dark:text-gray-400"> <a target="_blank" rel="noopener noreferrer" href="https://github.com/mdrathik/tailwind-nuxtjs-starter-blog"></a> </div> </div> </footer> </div> </section> </div> </body></html>